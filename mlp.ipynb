{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the data, then one-hot encoding the target variable (because it's categorical) and scaling the input variables (because they're continuous and on different scales, so we want to normalise them), finally splitting the data into training and testing sets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_onehot, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll now define the architecture of MLP\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 10\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# also initialising weights and biases\n",
    "# we use np.random.randn() to initialise the weights and np.zeros() to initialise the biases because it's a good practice to initialise the weights randomly and the biases to zero\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need an activation function, I have decided to go with ReLU - the purpose of this function, in simple terms, is to basically round off all negative values to zero and keep the positive values as they are\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# we will also need a softmax function for the output layer to get the probabilities of each class (3 in this case because of the iris dataset) - we use keepdims=True to keep the dimensions of the output the same as the input so that we can use it in the backpropagation step (which I will define later)\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# now we will define the forward propagation function which will take the input and return the output of the network, you can see that we are using the weights and biases that we initialised earlier and then each step is shown as per the architecture of the network and the defined activation functions for each layer\n",
    "def forward_propagation(X):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return A1, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of forward pass: [[8.91156965e-06 9.99147198e-01 8.43890830e-04]\n",
      " [5.69179183e-05 8.38383645e-01 1.61559437e-01]\n",
      " [6.36602721e-02 8.24455668e-02 8.53894161e-01]\n",
      " [4.93263922e-05 9.95415627e-01 4.53504678e-03]\n",
      " [1.76209943e-05 9.98712312e-01 1.27006743e-03]\n",
      " [3.75302528e-01 2.21604673e-01 4.03092800e-01]\n",
      " [7.06543580e-02 5.10117646e-02 8.78333877e-01]\n",
      " [2.69319528e-04 9.85215858e-01 1.45148224e-02]\n",
      " [6.12449484e-05 9.94804300e-01 5.13445501e-03]\n",
      " [1.05728739e-05 9.94459804e-01 5.52962273e-03]\n",
      " [4.87194324e-01 1.14081787e-01 3.98723890e-01]\n",
      " [9.97451226e-02 1.91558826e-01 7.08696052e-01]\n",
      " [5.35614364e-02 4.75562339e-02 8.98882330e-01]\n",
      " [1.59472379e-04 9.90660491e-01 9.18003622e-03]\n",
      " [3.01221907e-04 9.80415641e-01 1.92831368e-02]\n",
      " [1.27761659e-01 4.91732570e-01 3.80505771e-01]\n",
      " [2.75074121e-01 2.02186507e-01 5.22739372e-01]\n",
      " [5.83103073e-02 1.56644789e-02 9.26025214e-01]\n",
      " [1.08612802e-01 1.33663091e-01 7.57724107e-01]\n",
      " [6.84082825e-04 3.38097606e-04 9.98977820e-01]\n",
      " [3.17537441e-01 3.25222081e-01 3.57240478e-01]\n",
      " [2.96915641e-03 8.26782213e-04 9.96204061e-01]\n",
      " [4.04636517e-01 4.63426038e-01 1.31937445e-01]\n",
      " [1.67055732e-04 9.90551143e-01 9.28180112e-03]\n",
      " [6.35874801e-03 2.05505340e-03 9.91586199e-01]\n",
      " [1.60375713e-01 3.98720123e-01 4.40904164e-01]\n",
      " [4.54059789e-04 9.82826154e-01 1.67197861e-02]\n",
      " [1.51430560e-04 9.94496299e-01 5.35226997e-03]\n",
      " [6.72915965e-05 9.94118520e-01 5.81418890e-03]\n",
      " [6.74119112e-02 5.76068573e-01 3.56519516e-01]\n",
      " [2.73031005e-01 1.55956839e-01 5.71012157e-01]\n",
      " [4.23892641e-05 9.94163368e-01 5.79424265e-03]\n",
      " [9.37831191e-04 9.76032173e-01 2.30299959e-02]\n",
      " [1.10947870e-03 9.85860488e-01 1.30300331e-02]\n",
      " [2.98630860e-01 3.34308462e-01 3.67060678e-01]\n",
      " [2.54538435e-04 9.87158421e-01 1.25870404e-02]\n",
      " [2.36852260e-01 3.47775568e-01 4.15372172e-01]\n",
      " [6.57036138e-05 9.91789515e-06 9.99924378e-01]\n",
      " [4.85932470e-05 9.97064016e-01 2.88739037e-03]\n",
      " [2.86948477e-01 2.80854818e-01 4.32196704e-01]\n",
      " [4.95166822e-01 1.43165537e-01 3.61667641e-01]\n",
      " [4.43218400e-05 9.59449047e-01 4.05066309e-02]\n",
      " [2.30652776e-01 1.10966856e-01 6.58380368e-01]\n",
      " [4.87194324e-01 1.14081787e-01 3.98723890e-01]\n",
      " [2.78527647e-01 3.55486037e-01 3.65986316e-01]\n",
      " [2.41226171e-01 6.47127170e-01 1.11646659e-01]\n",
      " [2.23080507e-01 2.88020290e-01 4.88899203e-01]\n",
      " [3.30485342e-01 3.33499023e-01 3.36015635e-01]\n",
      " [2.16341264e-04 9.77892532e-01 2.18911270e-02]\n",
      " [7.78114050e-02 7.28862816e-01 1.93325779e-01]\n",
      " [2.10684176e-01 8.25462449e-02 7.06769579e-01]\n",
      " [2.53077154e-04 9.89686024e-01 1.00608988e-02]\n",
      " [4.49312171e-04 9.88857849e-01 1.06928388e-02]\n",
      " [1.82864276e-01 4.28912526e-01 3.88223198e-01]\n",
      " [5.60730839e-02 3.71127280e-02 9.06814188e-01]\n",
      " [3.64284547e-04 9.87809376e-01 1.18263396e-02]\n",
      " [6.25140267e-01 2.44157328e-01 1.30702406e-01]\n",
      " [1.49962895e-04 9.91124466e-01 8.72557109e-03]\n",
      " [2.76715674e-04 9.96455174e-01 3.26811039e-03]\n",
      " [3.53418761e-01 3.52411215e-01 2.94170024e-01]\n",
      " [1.52515623e-01 7.65305183e-01 8.21791938e-02]\n",
      " [5.39389520e-03 1.67702516e-03 9.92929080e-01]\n",
      " [4.05950648e-01 2.14915738e-01 3.79133614e-01]\n",
      " [3.33333333e-01 3.33333333e-01 3.33333333e-01]\n",
      " [1.88177566e-03 1.14396238e-03 9.96974262e-01]\n",
      " [3.11854969e-01 3.34211088e-01 3.53933943e-01]\n",
      " [6.95095531e-05 9.97218967e-01 2.71152380e-03]\n",
      " [2.01803916e-05 9.98685067e-01 1.29475240e-03]\n",
      " [3.33333333e-01 3.33333333e-01 3.33333333e-01]\n",
      " [5.89939777e-03 2.03306766e-03 9.92067535e-01]\n",
      " [7.42155066e-05 9.96000929e-01 3.92485504e-03]\n",
      " [2.44678960e-04 9.91366632e-01 8.38868937e-03]\n",
      " [4.54972581e-05 9.97960367e-01 1.99413597e-03]\n",
      " [3.64778443e-01 2.80585696e-01 3.54635862e-01]\n",
      " [1.51265007e-01 1.67947426e-01 6.80787567e-01]\n",
      " [2.75559025e-05 9.98310954e-01 1.66148973e-03]\n",
      " [2.23670120e-02 9.10590136e-03 9.68527087e-01]\n",
      " [9.52985462e-02 5.87414336e-02 8.45960020e-01]\n",
      " [8.69781543e-05 9.96064560e-01 3.84846188e-03]\n",
      " [3.04993685e-01 2.52459624e-01 4.42546691e-01]\n",
      " [2.04373474e-01 2.01195089e-01 5.94431437e-01]\n",
      " [2.48492420e-01 1.31303967e-01 6.20203613e-01]\n",
      " [2.20055077e-02 2.64160909e-02 9.51578401e-01]\n",
      " [6.96185062e-02 4.50821558e-02 8.85299338e-01]\n",
      " [3.65258113e-05 9.94388581e-01 5.57489350e-03]\n",
      " [1.20344509e-02 5.85966274e-03 9.82105886e-01]\n",
      " [3.29335346e-01 2.28886239e-01 4.41778416e-01]\n",
      " [5.30695396e-02 2.14370575e-02 9.25493403e-01]\n",
      " [1.82134380e-01 5.09975925e-01 3.07889695e-01]\n",
      " [3.80375424e-01 4.26970111e-01 1.92654465e-01]\n",
      " [2.51981703e-01 1.90988187e-01 5.57030110e-01]\n",
      " [1.19610743e-03 9.96546963e-01 2.25692922e-03]\n",
      " [1.65793264e-01 1.80317055e-01 6.53889680e-01]\n",
      " [1.91570633e-01 3.19738847e-01 4.88690520e-01]\n",
      " [1.77840306e-04 9.86426592e-01 1.33955674e-02]\n",
      " [2.53776471e-01 4.86726897e-01 2.59496632e-01]\n",
      " [1.61124196e-02 7.26081031e-03 9.76626770e-01]\n",
      " [8.54383347e-03 2.13203077e-03 9.89324136e-01]\n",
      " [1.32237682e-04 9.94617041e-01 5.25072102e-03]\n",
      " [7.65826441e-02 7.93382073e-01 1.30035283e-01]\n",
      " [1.09180482e-02 6.29210619e-03 9.82789846e-01]\n",
      " [2.01236128e-02 1.40723516e-02 9.65804036e-01]\n",
      " [2.45879740e-05 9.96044802e-01 3.93061044e-03]\n",
      " [1.11982206e-02 3.32772000e-03 9.85474059e-01]\n",
      " [6.14381014e-04 9.85757963e-01 1.36276564e-02]\n",
      " [3.39304805e-02 3.01563169e-02 9.35913203e-01]\n",
      " [1.64961399e-02 7.76475630e-03 9.75739104e-01]\n",
      " [1.49850035e-01 4.54476671e-02 8.04702298e-01]\n",
      " [3.37158871e-01 3.26809894e-01 3.36031236e-01]\n",
      " [6.86330335e-02 1.92089309e-02 9.12158036e-01]\n",
      " [3.23047260e-01 3.57610431e-01 3.19342309e-01]\n",
      " [2.19140275e-01 2.54520219e-01 5.26339506e-01]\n",
      " [6.48590434e-01 2.53186825e-01 9.82227414e-02]\n",
      " [2.97606165e-01 1.15774746e-01 5.86619089e-01]\n",
      " [1.10284644e-03 9.55286608e-01 4.36105458e-02]\n",
      " [3.10775572e-01 3.95151384e-01 2.94073044e-01]\n",
      " [6.05246101e-01 3.38158083e-01 5.65958160e-02]\n",
      " [3.98971152e-04 9.10377188e-01 8.92238409e-02]\n",
      " [3.28591844e-01 3.33601001e-01 3.37807156e-01]\n",
      " [1.05884629e-02 3.79869500e-03 9.85612842e-01]]\n"
     ]
    }
   ],
   "source": [
    "# with the functions defined above, we can do an example forward pass to see the output of the network\n",
    "A1, A2 = forward_propagation(X_train)\n",
    "print(\"Output of forward pass:\", A2)\n",
    "\n",
    "# the output of the network at this stage is not very useful because the weights and biases are random, but we can see that the output is a probability distribution over the 3 classes for each input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
