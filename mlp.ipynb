{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the data, then one-hot encoding the target variable (because it's categorical) and scaling the input variables (because they're continuous and on different scales, so we want to normalise them), finally splitting the data into training and testing sets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_onehot, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need an activation function, I have decided to go with ReLU - the purpose of this function, in simple terms, is to basically round off all negative values to zero and keep the positive values as they are\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# we will also need a softmax function for the output layer to get the probabilities of each class (3 in this case because of the iris dataset) - we use keepdims=True to keep the dimensions of the output the same as the input so that we can use it in the backpropagation step (which I will define later)\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# now we will define the forward propagation function which will take the input and return the output of the network, you can see that we are using the weights and biases that we initialised earlier and then each step is shown as per the architecture of the network and the defined activation functions for each layer\n",
    "def forward_propagation(X):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return A1, A2\n",
    "\n",
    "# now we will define the loss function, which is the cross-entropy loss function in this case, which is used for classification problems\n",
    "def compute_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll now define the architecture of MLP\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 10\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# also initialising weights and biases\n",
    "# we use np.random.randn() to initialise the weights and np.zeros() to initialise the biases because it's a good practice to initialise the weights randomly and the biases to zero\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of forward pass: [[1.00000000e+00 1.62049238e-10 1.08503351e-11]\n",
      " [9.70467781e-01 2.95322187e-02 1.01675898e-10]\n",
      " [8.40736681e-03 9.91174545e-01 4.18088133e-04]\n",
      " [9.99999996e-01 3.66163943e-09 6.78230390e-10]\n",
      " [1.00000000e+00 1.02661180e-10 7.63385131e-11]\n",
      " [9.18152382e-01 3.67866021e-02 4.50610158e-02]\n",
      " [5.86613847e-03 9.93484283e-01 6.49578504e-04]\n",
      " [9.99999984e-01 1.53033705e-08 7.21251001e-10]\n",
      " [9.99999994e-01 5.47681847e-09 2.34268368e-10]\n",
      " [9.99999163e-01 8.36703773e-07 4.25209602e-11]\n",
      " [7.12766285e-01 1.01394078e-01 1.85839636e-01]\n",
      " [3.55883745e-03 9.95613295e-01 8.27867556e-04]\n",
      " [5.51647493e-03 9.94180368e-01 3.03156982e-04]\n",
      " [9.99997975e-01 2.02452407e-06 1.75932654e-10]\n",
      " [9.99999895e-01 1.04781170e-07 6.63420967e-10]\n",
      " [9.85608755e-01 8.34115919e-04 1.35571290e-02]\n",
      " [2.93620260e-01 5.82515661e-01 1.23864079e-01]\n",
      " [6.19509787e-03 9.92915852e-01 8.89049720e-04]\n",
      " [4.26482318e-02 9.53854756e-01 3.49701198e-03]\n",
      " [1.25823886e-07 9.99999874e-01 9.45632506e-11]\n",
      " [4.73398487e-01 2.41315986e-01 2.85285527e-01]\n",
      " [4.97081753e-05 9.99950124e-01 1.68184843e-07]\n",
      " [3.91380415e-01 4.47855275e-01 1.60764311e-01]\n",
      " [9.99999993e-01 6.34697456e-09 3.34594507e-10]\n",
      " [3.74080094e-04 9.99623875e-01 2.04445708e-06]\n",
      " [7.03520784e-01 8.97729047e-02 2.06706312e-01]\n",
      " [9.99999992e-01 7.35538092e-09 4.77885620e-10]\n",
      " [9.99999995e-01 4.74926907e-09 2.88546062e-10]\n",
      " [9.99997259e-01 2.73632083e-06 5.05696752e-09]\n",
      " [9.99686975e-01 2.40060439e-06 3.10624811e-04]\n",
      " [7.65298803e-01 1.60496752e-01 7.42044447e-02]\n",
      " [9.99999981e-01 1.50193608e-08 3.75438781e-09]\n",
      " [9.99999995e-01 2.91097941e-09 1.90065756e-09]\n",
      " [9.99999893e-01 9.05045457e-08 1.63451490e-08]\n",
      " [6.81569220e-01 1.08183589e-01 2.10247191e-01]\n",
      " [9.99999991e-01 7.81691550e-09 6.93741377e-10]\n",
      " [3.56470829e-01 3.29046987e-01 3.14482184e-01]\n",
      " [8.40982638e-10 9.99999999e-01 9.46112184e-13]\n",
      " [9.99999999e-01 3.72839257e-10 2.00520287e-10]\n",
      " [2.87013707e-01 6.00385353e-01 1.12600940e-01]\n",
      " [8.04462821e-01 3.50460741e-02 1.60491105e-01]\n",
      " [9.99925103e-01 7.48972631e-05 7.37003929e-11]\n",
      " [2.05947601e-01 7.51224652e-01 4.28277468e-02]\n",
      " [7.12766285e-01 1.01394078e-01 1.85839636e-01]\n",
      " [9.73230405e-01 3.47570025e-03 2.32938950e-02]\n",
      " [4.65806003e-01 3.75808847e-01 1.58385150e-01]\n",
      " [2.54882897e-04 9.99739502e-01 5.61513320e-06]\n",
      " [9.70271245e-01 2.14609261e-03 2.75826622e-02]\n",
      " [9.99983637e-01 1.63606865e-05 1.92852161e-09]\n",
      " [9.99316176e-01 1.13717512e-05 6.72451928e-04]\n",
      " [6.21977165e-01 3.27287643e-01 5.07351921e-02]\n",
      " [9.99999997e-01 2.49435606e-09 3.77214289e-10]\n",
      " [9.99999999e-01 7.48947246e-10 1.41287921e-10]\n",
      " [9.78919010e-01 1.47798115e-03 1.96030091e-02]\n",
      " [1.40954089e-02 9.85364987e-01 5.39603628e-04]\n",
      " [9.99999998e-01 1.56918963e-09 7.52277535e-10]\n",
      " [4.34135267e-01 5.45955712e-01 1.99090206e-02]\n",
      " [9.99999994e-01 5.18967753e-09 5.69312349e-10]\n",
      " [9.99999800e-01 1.91742208e-07 8.41301344e-09]\n",
      " [2.92221357e-02 9.66759307e-01 4.01855762e-03]\n",
      " [9.98616032e-01 3.19344078e-05 1.35203334e-03]\n",
      " [1.65380186e-04 9.99834052e-01 5.67920901e-07]\n",
      " [5.93041269e-01 1.29962321e-01 2.76996410e-01]\n",
      " [2.93248791e-01 3.74051683e-01 3.32699527e-01]\n",
      " [6.34003727e-05 9.99936572e-01 2.80546193e-08]\n",
      " [9.31266643e-01 9.72219026e-03 5.90111666e-02]\n",
      " [1.00000000e+00 7.48448020e-11 2.17439685e-10]\n",
      " [1.00000000e+00 8.72704060e-12 1.47947555e-11]\n",
      " [7.88763468e-01 3.03830309e-02 1.80853501e-01]\n",
      " [3.48836397e-05 9.99964467e-01 6.49121220e-07]\n",
      " [9.99999999e-01 4.59187750e-10 4.15754643e-10]\n",
      " [9.99999988e-01 1.14510974e-08 6.21989100e-10]\n",
      " [1.00000000e+00 6.07705244e-11 1.04872113e-10]\n",
      " [7.18282715e-01 1.79520689e-01 1.02196595e-01]\n",
      " [1.31695588e-04 9.99866520e-01 1.78454445e-06]\n",
      " [9.99999999e-01 1.13635821e-09 2.71335092e-10]\n",
      " [4.91625641e-03 9.95036056e-01 4.76880152e-05]\n",
      " [3.05775471e-04 9.99689040e-01 5.18479681e-06]\n",
      " [1.00000000e+00 3.50825688e-10 1.38128941e-10]\n",
      " [3.04097182e-01 4.72896743e-01 2.23006075e-01]\n",
      " [3.45530593e-01 6.21327587e-01 3.31418200e-02]\n",
      " [6.26085761e-01 3.04067043e-01 6.98471952e-02]\n",
      " [4.42300448e-04 9.99546722e-01 1.09780052e-05]\n",
      " [2.01378387e-03 9.97965265e-01 2.09508138e-05]\n",
      " [9.99999905e-01 9.45680222e-08 3.93748117e-10]\n",
      " [9.64480581e-04 9.99030141e-01 5.37813421e-06]\n",
      " [3.62990085e-01 4.65932089e-01 1.71077826e-01]\n",
      " [2.34826188e-02 9.75883602e-01 6.33778790e-04]\n",
      " [9.57496619e-01 3.91691530e-03 3.85864660e-02]\n",
      " [9.26610658e-01 1.74126759e-02 5.59766656e-02]\n",
      " [1.82683943e-01 7.00357727e-01 1.16958330e-01]\n",
      " [9.99999999e-01 1.07616113e-11 6.46946219e-10]\n",
      " [6.30302644e-02 9.26120654e-01 1.08490812e-02]\n",
      " [7.82596460e-01 6.85300055e-02 1.48873535e-01]\n",
      " [9.99999942e-01 5.70278074e-08 5.25616240e-10]\n",
      " [4.52544151e-01 2.70926627e-01 2.76529222e-01]\n",
      " [7.27906043e-04 9.99251047e-01 2.10470056e-05]\n",
      " [9.40880504e-05 9.99904392e-01 1.51950857e-06]\n",
      " [9.99999833e-01 1.66122756e-07 8.49319412e-10]\n",
      " [9.99277007e-01 1.65953119e-05 7.06397775e-04]\n",
      " [9.24228664e-05 9.99907333e-01 2.44570491e-07]\n",
      " [4.76345763e-04 9.99501676e-01 2.19780880e-05]\n",
      " [9.99999998e-01 2.01536145e-09 1.03419561e-10]\n",
      " [1.62134473e-03 9.98372281e-01 6.37393317e-06]\n",
      " [9.99999999e-01 7.54212112e-10 4.98443140e-10]\n",
      " [1.81700868e-03 9.98114894e-01 6.80977117e-05]\n",
      " [3.01970907e-03 9.96953600e-01 2.66914199e-05]\n",
      " [1.08079611e-01 8.63553038e-01 2.83673506e-02]\n",
      " [5.87200427e-01 1.53454426e-01 2.59345147e-01]\n",
      " [1.61565418e-02 9.82120358e-01 1.72309976e-03]\n",
      " [8.51528805e-01 9.13745325e-02 5.70966620e-02]\n",
      " [1.81326236e-01 7.82001929e-01 3.66718349e-02]\n",
      " [6.38210685e-01 2.74544067e-01 8.72452482e-02]\n",
      " [1.87572319e-01 7.72807252e-01 3.96204291e-02]\n",
      " [9.99999928e-01 6.84496311e-08 3.92807541e-09]\n",
      " [5.73828036e-01 2.82377525e-01 1.43794439e-01]\n",
      " [9.41447428e-01 1.52132732e-02 4.33392988e-02]\n",
      " [9.99973312e-01 2.66879963e-05 2.87024799e-11]\n",
      " [7.73434590e-01 4.77171826e-02 1.78848227e-01]\n",
      " [8.94217803e-04 9.99100052e-01 5.73042984e-06]]\n",
      "Cross-entropy loss: 3.68048829202141\n"
     ]
    }
   ],
   "source": [
    "# with the functions defined above, we can do an example forward pass to see the output of the network\n",
    "A1, A2 = forward_propagation(X_train)\n",
    "print(\"Output of forward pass:\", A2)\n",
    "\n",
    "# we can also calculate the loss of the network using the loss function defined above, but this will be a random value because the weights and biases are initialised randomly, it is setup to show that the loss function is working correctly and will be used later to optimise the network\n",
    "loss = compute_loss(y_train, A2)\n",
    "print(\"Cross-entropy loss:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
