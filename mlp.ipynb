{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# loading in the data, then one-hot encoding the target variable (because it's categorical) and scaling the input variables (because they're continuous and on different scales, so we want to normalise them), finally splitting the data into training and testing sets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_onehot, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# we will need an activation function, I have decided to go with ReLU - the purpose of this function, in simple terms, is to basically round off all negative values to zero and keep the positive values as they are\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# we will also need a softmax function for the output layer to get the probabilities of each class (3 in this case because of the iris dataset) - we use keepdims=True to keep the dimensions of the output the same as the input so that we can use it in the backpropagation step (which I will define later)\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# now we will define the forward propagation function which will take the input and return the output of the network, you can see that we are using the weights and biases that we initialised earlier and then each step is shown as per the architecture of the network and the defined activation functions for each layer\n",
    "def forward_propagation(X):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return A1, A2\n",
    "\n",
    "# now we will define the loss function, which is the cross-entropy loss function in this case, which is used for classification problems\n",
    "def compute_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / m\n",
    "    return loss\n",
    "\n",
    "# now we will define the backpropagation function which will take the input and the output of the network and return the gradients of the weights and biases\n",
    "def backward_propagation(X, y_true, A1, A2):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    dZ2 = A2 - y_true\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "    \n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * (A1 > 0)\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# now we will define the function to update the parameters of the network using the gradients and the learning rate - this works based on the idea of gradient descent, where we update the parameters in the opposite direction of the gradient to minimise the loss function - this is grounded in the idea of calculus, where the gradient of a function at a point gives the direction of the steepest ascent of the function at that point - chain rule is used to calculate the gradients of the weights and biases\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# now we will define the training function which will take the input, target, number of epochs and learning rate and train the network using the functions we defined earlier - this will simply run the forward and backward propagation functions for the number of epochs specified and print the loss every 100 epochs - at the end of the training, we will have the weights and biases of the network that we can use to make predictions\n",
    "# Training loop\n",
    "def train(X_train, y_train, epochs_array, learning_rate_array):\n",
    "    global W1, b1, W2, b2\n",
    "    loss_values = []\n",
    "    for epochs in epochs_array:\n",
    "        for learning_rate in learning_rate_array:\n",
    "            W1 = np.random.randn(input_size, hidden_size)\n",
    "            b1 = np.zeros((1, hidden_size))\n",
    "            W2 = np.random.randn(hidden_size, output_size)\n",
    "            b2 = np.zeros((1, output_size))\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                A1, A2 = forward_propagation(X_train)\n",
    "                loss = compute_loss(y_train, A2)\n",
    "                dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, A1, A2)\n",
    "                W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "                \n",
    "                if epoch % 100 == 0 or epoch == 0 or epoch == epochs - 1:\n",
    "                    if epoch == 0:\n",
    "                        print(f\"Initial Loss:    {loss:.4f}\")\n",
    "                        continue\n",
    "                    elif epoch == epochs - 1:\n",
    "                        print(f\"Final Loss:      {loss:.4f}\")\n",
    "                        print()\n",
    "                        continue\n",
    "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            loss_values.append((epochs, learning_rate, loss))\n",
    "    return loss_values\n",
    "\n",
    "# now we will define the function to make predictions using the trained network, which will take the input and return the predicted class\n",
    "def predict(X):\n",
    "    _, A2 = forward_propagation(X)\n",
    "    return np.argmax(A2, axis=1)\n",
    "\n",
    "# now we will define the function to calculate the accuracy of the model, which will take the input and the true target values and return the accuracy of the model\n",
    "def calculate_accuracy(X, y_true):\n",
    "    y_pred = predict(X)\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    accuracy = np.mean(y_pred == y_true_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP ARCHITECTURE\n",
    "\n",
    "# we'll now define the architecture of MLP\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 10\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# also initialising weights and biases\n",
    "# we use np.random.randn() to initialise the weights and np.zeros() to initialise the biases because it's a good practice to initialise the weights randomly and the biases to zero\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss:    4.8960\n",
      "Epoch 100, Loss: 0.4157\n",
      "Epoch 200, Loss: 0.3510\n",
      "Epoch 300, Loss: 0.3173\n",
      "Epoch 400, Loss: 0.2924\n",
      "Final Loss:      0.2723\n",
      "\n",
      "Initial Loss:    2.2795\n",
      "Epoch 100, Loss: 1.6118\n",
      "Epoch 200, Loss: 1.2032\n",
      "Epoch 300, Loss: 0.9702\n",
      "Epoch 400, Loss: 0.8234\n",
      "Final Loss:      0.7228\n",
      "\n",
      "Initial Loss:    2.3898\n",
      "Epoch 100, Loss: 0.5765\n",
      "Epoch 200, Loss: 0.3894\n",
      "Epoch 300, Loss: 0.3207\n",
      "Epoch 400, Loss: 0.2768\n",
      "Epoch 500, Loss: 0.2452\n",
      "Epoch 600, Loss: 0.2208\n",
      "Epoch 700, Loss: 0.2010\n",
      "Epoch 800, Loss: 0.1840\n",
      "Epoch 900, Loss: 0.1693\n",
      "Final Loss:      0.1568\n",
      "\n",
      "Initial Loss:    3.0284\n",
      "Epoch 100, Loss: 2.4916\n",
      "Epoch 200, Loss: 2.0205\n",
      "Epoch 300, Loss: 1.6131\n",
      "Epoch 400, Loss: 1.2799\n",
      "Epoch 500, Loss: 1.0132\n",
      "Epoch 600, Loss: 0.8160\n",
      "Epoch 700, Loss: 0.6798\n",
      "Epoch 800, Loss: 0.5883\n",
      "Epoch 900, Loss: 0.5290\n",
      "Final Loss:      0.4891\n",
      "\n",
      "Training Accuracy: 81.67%\n",
      "Test Accuracy: 90.00%\n",
      "Loss values for each combination of epochs and learning rate:\n",
      "[(500, 0.01, 0.272325183934216), (500, 0.001, 0.7227649225170645), (1000, 0.01, 0.15676053162429812), (1000, 0.001, 0.4890516825588873)]\n",
      "\n",
      "Lowest loss value: 0.1568 for 1000 epochs and learning rate 0.01\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "\n",
    "# cut down most of the previous comments - now all we need to do is call the train function with the training data, number of epochs and learning rate - the rest is handled by logic defined earlier\n",
    "epochs_array = [500, 1000]\n",
    "learning_rate_array = [0.01, 0.001]\n",
    "loss_values = train(X_train, y_train, epochs_array, learning_rate_array)\n",
    "\n",
    "# EVALUATION\n",
    "train_accuracy = calculate_accuracy(X_train, y_train)\n",
    "test_accuracy = calculate_accuracy(X_test, y_test)\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print loss values\n",
    "print(\"Loss values for each combination of epochs and learning rate:\")\n",
    "print(loss_values)\n",
    "print()\n",
    "\n",
    "# Lowest loss value\n",
    "min_loss = min(loss_values, key=lambda x: x[2])\n",
    "print(f\"Lowest loss value: {min_loss[2]:.4f} for {min_loss[0]} epochs and learning rate {min_loss[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
